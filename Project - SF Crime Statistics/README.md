1. How did changing values on the SparkSession property parameters affect the throughput and latency of the data?
- I can improve upon the speed of the processing by assigning more memory to both the driver and the executor of the cluster via the `spark.driver.memory` and `spark.executor.memory` config options. Additionally, I can increase the number of cores and parallelism of the process using the `spark.driver.cores`, `spark.executor.cores`, `spark.default.parallelism`. The outcome is a higher throughput though there is more network overhead involved when more distributed servers are involved. In this case keeping the number of cores to the default of 1 is sufficient. Furthermore, the `processingTime` parameter of the `trigger` function of `writeStream` can be reduced to increase throughput. The `maxRatePerPartition` option of `readStream` can also be tweaked to achieve the desired throughput. 

2. What were the 2-3 most efficient SparkSession property key/value pairs? Through testing multiple variations on values, how can you tell these were the most optimal?
- Based on testing, increasing the `spark.driver.memory` and `spark.executor.memory` to "2g" or 2 gigabytes improved processing speed. Keeping `spark.driver.cores` and `spark.executor.cores` to the default was optimal. Finally keeping the `maxRatePerPartition` option of readStream to roughly 200 was sufficient for the task. 